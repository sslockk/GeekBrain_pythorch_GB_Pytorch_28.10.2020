{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание по итогам второго вебинара.\n",
    "1. Приложен ноутбук, в котором реализованы функции для генирации из большого\n",
    "датасета меньшая его копия. Вам нужно перенести функции из этого ноутбука в\n",
    "класс датасет и сделать следующее:\n",
    "a. Сгенерировать меньший датасет из 8-10 классов движения\n",
    "b. Обучить уже существующую модель (предварительно проанализировав\n",
    "какие параметры модели нужно изменить)\n",
    "c. Изменить модель: посмотреть зависимость от количества LSTM модулей\n",
    "в нашей модели\n",
    "d. Сгенерировать другой датасет с меньшим количеством “кадров” в серии\n",
    "и сравнить улучшилось или ухудшилось качество предсказания.\n",
    "Провести несколько таких итераций, дать свою оценку уменьшению и\n",
    "увеличению кадров, назвать оптимальное, на ваш взгляд, их количество.\n",
    "Желательно сделать так, чтобы длина последовательности\n",
    "передавалась как атрибут класса.\n",
    "2. Дополнительное задание:\n",
    "a. http://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+c\n",
    "onsumption - 2075259 measurements gathered in a house located in Sceaux\n",
    "(7km of Paris, France) between December 2006 and November 2010 (47\n",
    "months). Проделайте весь путь подготовки данных, создания датасета,\n",
    "разделения и обучения модели самостоятельно. Предсказывать нужно\n",
    "Global_active_power. Обратите внимание, что здесь задача регрессии, а\n",
    "не классификации, т.е. модель нужно изменить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch import optim\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Сгенерировать меньший датасет из 8-10 классов движения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"E:/IT_WORK/!!!_GB_Pytorch_28.10.2020/Lession02/nturgb+d_skeletons/\"\n",
    "broken_files_path = \"NTU_RGBD_samples_with_missing_skeletons.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_subjects = list(range(0, 28)) #количество людей выполняющих действия\n",
    "training_classes = sorted([8, 10, 22, 23, 27, 21, 55, 2, 7]) #классы которые будем использовать для обучения, полный список прдставлен тут https://github.com/shahroudy/NTURGB-D\n",
    "LABELS = {x: training_classes[x] for x in range(len(training_classes))}\n",
    "training_cameras = [1, 2, 3] \n",
    "\n",
    "# max_body_true = 1\n",
    "# max_body_kinect = 1\n",
    "\n",
    "num_joint = 25\n",
    "max_frame = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skeleton_Dataset(Dataset):\n",
    "    def __init__(self, data_path, broken_files_path=None, training_classes=None,\n",
    "                 num_joint = 25, max_frame = 300, transform=None):\n",
    "        \n",
    "        \n",
    "        def read_data(data_path, broken_files_path):\n",
    "            labels = []\n",
    "            files = []\n",
    "            action_classes = {}\n",
    "            counter = 0\n",
    "            files_counter = {}\n",
    "            with open(broken_files_path, 'r') as f:\n",
    "                broken_files = f.read().split(\"\\n\")\n",
    "\n",
    "            raw_files = os.listdir(data_path)\n",
    "            num_frames = 0\n",
    "\n",
    "            for filename in raw_files:\n",
    "                if filename not in broken_files:\n",
    "                    action_class = int(filename[filename.find('A') + 1:filename.find('A') + 4])\n",
    "                    subject_id = int(filename[filename.find('P') + 1:filename.find('P') + 4])\n",
    "                    camera_id = int(filename[filename.find('C') + 1:filename.find('C') + 4])\n",
    "                    if action_class in training_classes and camera_id in training_cameras: \n",
    "                        if action_class in action_classes:\n",
    "                            if files_counter[action_class] < 120:\n",
    "                                files.append([filename,action_classes[action_class]])\n",
    "                                files_counter[action_class] = files_counter[action_class] + 1\n",
    "                        else:\n",
    "                            action_classes.update({action_class : counter})\n",
    "                            files_counter.update({action_class : 1})\n",
    "                            counter+=1\n",
    "                            files.append([filename,action_classes[action_class]])\n",
    "            print(\"action classes: \", action_classes)\n",
    "            print(\"action files: \", files_counter)\n",
    "\n",
    "            return files, action_classes\n",
    "        \n",
    "        \n",
    "        def read_skeleton_filter(file):\n",
    "            with open(file, 'r') as f:\n",
    "                skeleton_sequence = {}\n",
    "                skeleton_sequence['numFrame'] = int(f.readline())\n",
    "                skeleton_sequence['frameInfo'] = []\n",
    "                for t in range(skeleton_sequence['numFrame']):\n",
    "                    frame_info = {}\n",
    "                    frame_info['numBody'] = int(f.readline())\n",
    "                    frame_info['bodyInfo'] = []\n",
    "\n",
    "                    for m in range(frame_info['numBody']):\n",
    "                        body_info = {}\n",
    "                        body_info_key = [\n",
    "                            'bodyID', 'clipedEdges', 'handLeftConfidence',\n",
    "                            'handLeftState', 'handRightConfidence', 'handRightState',\n",
    "                            'isResticted', 'leanX', 'leanY', 'trackingState'\n",
    "                        ]\n",
    "                        body_info = {\n",
    "                            k: float(v)\n",
    "                            for k, v in zip(body_info_key, f.readline().split())\n",
    "                        }\n",
    "                        body_info['numJoint'] = int(f.readline())\n",
    "                        body_info['jointInfo'] = []\n",
    "                        for v in range(body_info['numJoint']):\n",
    "                            joint_info_key = [\n",
    "                                'x', 'y', 'z', 'depthX', 'depthY', 'colorX', 'colorY',\n",
    "                                'orientationW', 'orientationX', 'orientationY',\n",
    "                                'orientationZ', 'trackingState'\n",
    "                            ]\n",
    "                            joint_info = {\n",
    "                                k: float(v)\n",
    "                                for k, v in zip(joint_info_key, f.readline().split())\n",
    "                            }\n",
    "                            body_info['jointInfo'].append(joint_info)\n",
    "                        frame_info['bodyInfo'].append(body_info)\n",
    "                    skeleton_sequence['frameInfo'].append(frame_info)\n",
    "\n",
    "            return skeleton_sequence\n",
    "\n",
    "        def read_xyz(file, max_body=1, num_joint=25):\n",
    "            seq_info = read_skeleton_filter(file)\n",
    "            data = np.zeros((max_body, seq_info['numFrame'], num_joint, 3))\n",
    "            for n, f in enumerate(seq_info['frameInfo']):\n",
    "                for m, b in enumerate(f['bodyInfo']):\n",
    "                    for j, v in enumerate(b['jointInfo']):\n",
    "                        if m < max_body and j < num_joint:\n",
    "                            data[m, n, j, :] = [v['x'], v['y'], v['z']]\n",
    "\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "            return data\n",
    "        \n",
    "        \n",
    "        def create_coords_blocks(test_file, chonk_len = 45):   \n",
    "            frame_counter = 0\n",
    "            new_labels = []\n",
    "            new_frames = []\n",
    "            blocks = []\n",
    "\n",
    "            test_frames = read_xyz(data_path + test_file[0])[0]\n",
    "            label = test_file[1]\n",
    "            slice_len = chonk_len * int(len(test_frames)/chonk_len)\n",
    "\n",
    "\n",
    "            for index in range(len(test_frames[:slice_len])):\n",
    "                frame_counter += 1\n",
    "                new_frames.append(test_frames[index].flatten())\n",
    "                if frame_counter == chonk_len:\n",
    "                    frame_counter = 0\n",
    "                    blocks.append(np.array(new_frames))\n",
    "                    new_labels = new_labels + [label]\n",
    "                    new_frames = []\n",
    "\n",
    "\n",
    "            return blocks, new_labels\n",
    "        \n",
    "        \n",
    "        ##### список файлов с лейблами на каждый файл \n",
    "        working_files_with_labels, action_classes = read_data(data_path, broken_files_path)\n",
    "        \n",
    "        data = []\n",
    "        labels = []\n",
    "        ##########################################################################\n",
    "        numbers = {x: 0 for x in range(len(action_classes))}  #####\n",
    "        ##################################################################\n",
    "        for file in working_files_with_labels:\n",
    "            frames_blocks, label = create_coords_blocks(file)\n",
    "            if label != [] and numbers[label[0]] <= 150:\n",
    "                numbers[label[0]] = numbers[label[0]] + len(label)\n",
    "                data = data + frames_blocks\n",
    "                labels = labels + label\n",
    "        data_np = np.asarray(data)\n",
    "        labels_np = np.asarray(labels)\n",
    "\n",
    "        data_sq = data_np.reshape(len(data_np), -1)\n",
    "        data = pd.DataFrame(data_sq)\n",
    "        labels = pd.DataFrame(labels_np)\n",
    "        data['labels'] = labels\n",
    "        \n",
    "\n",
    "        self.data = data\n",
    "        self.labels = data['labels'].astype('float32')\n",
    "        self.transform = transform\n",
    "        \n",
    "           \n",
    "    def __len__(self):\n",
    "         return len(self.data)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = np.asarray(self.data.iloc[idx,:-1]).reshape(45,75)\n",
    "        label = self.labels[idx]\n",
    "        if self.transform != None:\n",
    "            item = transform(item)\n",
    "        return (item, label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action classes:  {2: 0, 7: 1, 8: 2, 10: 3, 21: 4, 22: 5, 23: 6, 27: 7, 55: 8}\n",
      "action files:  {2: 120, 7: 120, 8: 120, 10: 120, 21: 120, 22: 120, 23: 120, 27: 120, 55: 120}\n"
     ]
    }
   ],
   "source": [
    "dataset = Skeleton_Dataset(data_path=data_path, broken_files_path=broken_files_path, \n",
    "                           training_classes=training_classes,num_joint = 25, \n",
    "                           max_frame = 300, transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Обучить уже существующую модель (предварительно проанализировав какие параметры модели нужно изменить) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(0.75*len(dataset)),\n",
    "                                                                      len(dataset) - int(0.75*len(dataset))])\n",
    "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_net(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,output_dim,layer_num):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lstm = torch.nn.LSTM(input_dim, hidden_dim,layer_num,batch_first=True)\n",
    "        self.dr = torch.nn.Dropout2d(0.1)\n",
    "        self.fc = torch.nn.Linear(hidden_dim,output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x = inputs\n",
    "        lstm_out,(hn,cn) = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_net(\n",
       "  (lstm): LSTM(75, 128, num_layers=2, batch_first=True)\n",
       "  (dr): Dropout2d(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=128, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hidden = 128\n",
    "n_joints = 25*3\n",
    "n_categories = len(LABELS)\n",
    "n_layer = 2\n",
    "rnn = LSTM_net(n_joints,n_hidden,n_categories,n_layer)\n",
    "rnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "#     print(output.topk(5))\n",
    "    return LABELS[category_i], category_i\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 iter : 0 (0m 2s) 2.2490  / 22 ✗ (55)\n",
      "epoch : 8 iter : 36 (0m 19s) 2.2011  / 2 ✗ (7)\n",
      "epoch : 17 iter : 14 (0m 36s) 2.1531  / 2 ✗ (8)\n",
      "epoch : 25 iter : 50 (0m 54s) 2.1022  / 2 ✗ (22)\n",
      "epoch : 34 iter : 28 (1m 11s) 1.6837  / 2 ✗ (27)\n",
      "epoch : 43 iter : 6 (1m 28s) 2.0363  / 2 ✗ (7)\n",
      "epoch : 51 iter : 42 (1m 45s) 1.8114  / 2 ✗ (7)\n",
      "epoch : 60 iter : 20 (2m 3s) 1.5551  / 23 ✓\n",
      "epoch : 68 iter : 56 (2m 20s) 1.4552  / 55 ✗ (27)\n",
      "epoch : 77 iter : 34 (2m 38s) 1.2948  / 2 ✓\n",
      "epoch : 86 iter : 12 (2m 55s) 1.9694  / 7 ✗ (55)\n",
      "epoch : 94 iter : 48 (3m 14s) 1.0268  / 27 ✓\n",
      "epoch : 103 iter : 26 (3m 32s) 1.4869  / 2 ✓\n",
      "epoch : 112 iter : 4 (3m 50s) 1.2797  / 22 ✓\n",
      "epoch : 120 iter : 40 (4m 8s) 1.0014  / 55 ✓\n",
      "epoch : 129 iter : 18 (4m 25s) 0.8151  / 8 ✓\n",
      "epoch : 137 iter : 54 (4m 43s) 1.0890  / 7 ✗ (21)\n",
      "epoch : 146 iter : 32 (5m 0s) 0.5559  / 7 ✗ (23)\n",
      "epoch : 155 iter : 10 (5m 17s) 0.8957  / 2 ✓\n",
      "epoch : 163 iter : 46 (5m 34s) 0.7047  / 27 ✓\n",
      "epoch : 172 iter : 24 (5m 51s) 1.0431  / 27 ✗ (23)\n",
      "epoch : 181 iter : 2 (6m 8s) 0.6388  / 55 ✓\n",
      "epoch : 189 iter : 38 (6m 25s) 1.1040  / 21 ✗ (23)\n",
      "epoch : 198 iter : 16 (6m 43s) 1.1599  / 55 ✓\n",
      "epoch : 206 iter : 52 (7m 0s) 0.8007  / 2 ✓\n",
      "epoch : 215 iter : 30 (7m 17s) 1.3538  / 55 ✗ (10)\n",
      "epoch : 224 iter : 8 (7m 35s) 0.5460  / 2 ✓\n",
      "epoch : 232 iter : 44 (7m 52s) 0.6631  / 22 ✗ (21)\n",
      "epoch : 241 iter : 22 (8m 9s) 0.6750  / 22 ✓\n",
      "epoch : 250 iter : 0 (8m 27s) 0.4587  / 27 ✓\n",
      "epoch : 258 iter : 36 (8m 44s) 0.6883  / 23 ✓\n",
      "epoch : 267 iter : 14 (9m 2s) 1.8358  / 22 ✓\n",
      "epoch : 275 iter : 50 (9m 19s) 1.3275  / 55 ✗ (2)\n",
      "epoch : 284 iter : 28 (9m 36s) 0.8052  / 8 ✓\n",
      "epoch : 293 iter : 6 (9m 55s) 0.7245  / 23 ✓\n",
      "epoch : 301 iter : 42 (10m 13s) 1.2560  / 21 ✗ (7)\n",
      "epoch : 310 iter : 20 (10m 31s) 0.7068  / 27 ✓\n",
      "epoch : 318 iter : 56 (10m 49s) 0.8605  / 55 ✓\n",
      "epoch : 327 iter : 34 (11m 7s) 0.4074  / 8 ✓\n",
      "epoch : 336 iter : 12 (11m 24s) 0.6398  / 7 ✓\n",
      "epoch : 344 iter : 48 (11m 43s) 0.6326  / 22 ✓\n",
      "epoch : 353 iter : 26 (12m 0s) 0.3338  / 8 ✓\n",
      "epoch : 362 iter : 4 (12m 18s) 0.5049  / 7 ✗ (2)\n",
      "epoch : 370 iter : 40 (12m 35s) 0.4832  / 10 ✓\n",
      "epoch : 379 iter : 18 (12m 52s) 0.5996  / 2 ✗ (7)\n",
      "epoch : 387 iter : 54 (13m 10s) 0.8712  / 10 ✗ (2)\n",
      "epoch : 396 iter : 32 (13m 28s) 0.5024  / 55 ✗ (8)\n",
      "epoch : 405 iter : 10 (13m 46s) 0.1791  / 2 ✓\n",
      "epoch : 413 iter : 46 (14m 3s) 0.5378  / 21 ✗ (23)\n",
      "epoch : 422 iter : 24 (14m 22s) 0.4089  / 8 ✓\n",
      "epoch : 431 iter : 2 (14m 40s) 0.0651  / 10 ✓\n",
      "epoch : 439 iter : 38 (14m 58s) 0.3078  / 10 ✓\n",
      "epoch : 448 iter : 16 (15m 16s) 0.1163  / 8 ✓\n",
      "epoch : 456 iter : 52 (15m 34s) 0.0358  / 23 ✓\n",
      "epoch : 465 iter : 30 (15m 52s) 0.0265  / 27 ✓\n",
      "epoch : 474 iter : 8 (16m 10s) 0.7083  / 55 ✓\n",
      "epoch : 482 iter : 44 (16m 28s) 0.0602  / 2 ✓\n",
      "epoch : 491 iter : 22 (16m 46s) 0.3826  / 27 ✓\n",
      "epoch : 500 iter : 0 (17m 3s) 0.0253  / 2 ✓\n",
      "epoch : 508 iter : 36 (17m 21s) 0.5835  / 2 ✓\n",
      "epoch : 517 iter : 14 (17m 39s) 0.0333  / 22 ✓\n",
      "epoch : 525 iter : 50 (17m 57s) 0.0439  / 8 ✓\n",
      "epoch : 534 iter : 28 (18m 14s) 0.0132  / 27 ✓\n",
      "epoch : 543 iter : 6 (18m 32s) 0.0166  / 21 ✓\n",
      "epoch : 551 iter : 42 (18m 49s) 0.0117  / 8 ✓\n",
      "epoch : 560 iter : 20 (19m 7s) 0.0902  / 27 ✓\n",
      "epoch : 568 iter : 56 (19m 25s) 0.1087  / 22 ✓\n",
      "epoch : 577 iter : 34 (19m 43s) 0.0152  / 27 ✓\n",
      "epoch : 586 iter : 12 (20m 0s) 0.0327  / 23 ✓\n",
      "epoch : 594 iter : 48 (20m 17s) 0.0243  / 22 ✓\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.0007\n",
    "optimizer = optim.SGD(rnn.parameters(),lr=learning_rate,momentum=0.9)\n",
    "\n",
    "all_losses = []\n",
    "start = time.time()\n",
    "counter = 0\n",
    "for epoch in range(600):  \n",
    "    current_loss = 0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        output = rnn(inputs.float())\n",
    "        labels = labels.type(torch.LongTensor).to(device)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        category = LABELS[int(labels[0])]\n",
    "\n",
    "        if counter % 500 == 0:\n",
    "            guess, guess_i = categoryFromOutput(output)\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "            print('epoch : %d iter : %d (%s) %.4f  / %s %s' % (epoch, i, timeSince(start), loss, guess, correct))\n",
    "\n",
    "        \n",
    "        counter = counter + 1\n",
    "    if counter % 100 == 0:\n",
    "        all_losses.append(current_loss / 25)\n",
    "        current_loss = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network:   73.13915857605178\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "right = 0\n",
    "counter = 0\n",
    "\n",
    "rnn.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        counter = counter + 1\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)  \n",
    "        output = rnn(inputs.float())\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        category = LABELS[int(labels[0])]\n",
    "        \n",
    "        if guess == category:\n",
    "            right = right + 1\n",
    "\n",
    "\n",
    "print('Accuracy of the network:  ',  (100 * right / counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Изменить модель: посмотреть зависимость от количества LSTM модулей в нашей модели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_net(\n",
       "  (lstm): LSTM(75, 384, num_layers=2, batch_first=True)\n",
       "  (dr): Dropout2d(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=384, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hidden = 128*3\n",
    "n_joints = 25*3\n",
    "n_categories = len(LABELS)\n",
    "n_layer = 2\n",
    "rnn = LSTM_net(n_joints,n_hidden,n_categories,n_layer)\n",
    "rnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_net(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,output_dim,layer_num):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lstm = torch.nn.LSTM(input_dim, hidden_dim,layer_num,batch_first=True)\n",
    "        self.dr = torch.nn.Dropout2d(0.1)\n",
    "        self.fc = torch.nn.Linear(hidden_dim,output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x = inputs\n",
    "        lstm_out,(hn,cn) = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 iter : 0 (0m 0s) 2.1823  / 10 ✗ (27)\n",
      "epoch : 8 iter : 36 (0m 18s) 2.1942  / 2 ✗ (21)\n",
      "epoch : 17 iter : 14 (0m 37s) 2.1744  / 2 ✗ (7)\n",
      "epoch : 25 iter : 50 (0m 55s) 1.9351  / 22 ✗ (23)\n",
      "epoch : 34 iter : 28 (1m 14s) 1.7881  / 22 ✓\n",
      "epoch : 43 iter : 6 (1m 33s) 1.7395  / 21 ✓\n",
      "epoch : 51 iter : 42 (1m 51s) 1.9025  / 2 ✓\n",
      "epoch : 60 iter : 20 (2m 10s) 1.3851  / 2 ✓\n",
      "epoch : 68 iter : 56 (2m 30s) 1.7453  / 22 ✓\n",
      "epoch : 77 iter : 34 (2m 48s) 1.2845  / 2 ✗ (7)\n",
      "epoch : 86 iter : 12 (3m 6s) 1.1206  / 7 ✗ (23)\n",
      "epoch : 94 iter : 48 (3m 25s) 1.2961  / 7 ✓\n",
      "epoch : 103 iter : 26 (3m 45s) 0.9987  / 27 ✓\n",
      "epoch : 112 iter : 4 (4m 5s) 1.0895  / 7 ✗ (23)\n",
      "epoch : 120 iter : 40 (4m 24s) 0.9364  / 2 ✓\n",
      "epoch : 129 iter : 18 (4m 43s) 0.7155  / 22 ✓\n",
      "epoch : 137 iter : 54 (5m 2s) 0.7854  / 10 ✓\n",
      "epoch : 146 iter : 32 (5m 20s) 0.6873  / 21 ✗ (2)\n",
      "epoch : 155 iter : 10 (5m 39s) 0.8368  / 7 ✓\n",
      "epoch : 163 iter : 46 (5m 59s) 0.3618  / 27 ✓\n",
      "epoch : 172 iter : 24 (6m 17s) 0.4829  / 27 ✓\n",
      "epoch : 181 iter : 2 (6m 36s) 0.4302  / 55 ✓\n",
      "epoch : 189 iter : 38 (6m 54s) 1.2521  / 21 ✓\n",
      "epoch : 198 iter : 16 (7m 14s) 0.5587  / 21 ✗ (23)\n",
      "epoch : 206 iter : 52 (7m 34s) 0.4520  / 2 ✗ (8)\n",
      "epoch : 215 iter : 30 (7m 53s) 0.3081  / 23 ✗ (21)\n",
      "epoch : 224 iter : 8 (8m 13s) 0.2199  / 8 ✓\n",
      "epoch : 232 iter : 44 (8m 32s) 0.3680  / 27 ✓\n",
      "epoch : 241 iter : 22 (8m 52s) 0.0894  / 21 ✓\n",
      "epoch : 250 iter : 0 (9m 12s) 0.1005  / 23 ✓\n",
      "epoch : 258 iter : 36 (9m 33s) 0.1743  / 2 ✓\n",
      "epoch : 267 iter : 14 (9m 52s) 0.3494  / 23 ✗ (7)\n",
      "epoch : 275 iter : 50 (10m 11s) 0.6963  / 55 ✗ (2)\n",
      "epoch : 284 iter : 28 (10m 29s) 0.1216  / 10 ✓\n",
      "epoch : 293 iter : 6 (10m 49s) 0.0642  / 10 ✓\n",
      "epoch : 301 iter : 42 (11m 8s) 0.1206  / 55 ✓\n",
      "epoch : 310 iter : 20 (11m 26s) 0.2033  / 23 ✓\n",
      "epoch : 318 iter : 56 (11m 45s) 0.1739  / 22 ✓\n",
      "epoch : 327 iter : 34 (12m 4s) 0.0489  / 8 ✓\n",
      "epoch : 336 iter : 12 (12m 23s) 0.1366  / 8 ✓\n",
      "epoch : 344 iter : 48 (12m 42s) 0.1138  / 2 ✓\n",
      "epoch : 353 iter : 26 (13m 1s) 0.0272  / 2 ✓\n",
      "epoch : 362 iter : 4 (13m 20s) 0.4668  / 22 ✓\n",
      "epoch : 370 iter : 40 (13m 39s) 0.0512  / 21 ✓\n",
      "epoch : 379 iter : 18 (13m 58s) 0.0066  / 2 ✓\n",
      "epoch : 387 iter : 54 (14m 17s) 0.0832  / 23 ✓\n",
      "epoch : 396 iter : 32 (14m 35s) 0.0114  / 2 ✓\n",
      "epoch : 405 iter : 10 (14m 55s) 0.0104  / 22 ✓\n",
      "epoch : 413 iter : 46 (15m 14s) 0.1990  / 7 ✓\n",
      "epoch : 422 iter : 24 (15m 33s) 0.1260  / 23 ✓\n",
      "epoch : 431 iter : 2 (15m 51s) 0.0098  / 55 ✓\n",
      "epoch : 439 iter : 38 (16m 10s) 0.0072  / 21 ✓\n",
      "epoch : 448 iter : 16 (16m 28s) 0.0076  / 2 ✓\n",
      "epoch : 456 iter : 52 (16m 47s) 0.0047  / 10 ✓\n",
      "epoch : 465 iter : 30 (17m 6s) 0.0012  / 2 ✓\n",
      "epoch : 474 iter : 8 (17m 25s) 0.0056  / 27 ✓\n",
      "epoch : 482 iter : 44 (17m 43s) 0.0044  / 27 ✓\n",
      "epoch : 491 iter : 22 (18m 2s) 0.0023  / 8 ✓\n",
      "epoch : 500 iter : 0 (18m 20s) 0.0029  / 23 ✓\n",
      "epoch : 508 iter : 36 (18m 39s) 0.0012  / 21 ✓\n",
      "epoch : 517 iter : 14 (18m 57s) 0.0007  / 2 ✓\n",
      "epoch : 525 iter : 50 (19m 16s) 0.0025  / 10 ✓\n",
      "epoch : 534 iter : 28 (19m 35s) 0.0014  / 2 ✓\n",
      "epoch : 543 iter : 6 (19m 53s) 0.0031  / 22 ✓\n",
      "epoch : 551 iter : 42 (20m 12s) 0.0034  / 23 ✓\n",
      "epoch : 560 iter : 20 (20m 30s) 0.0048  / 27 ✓\n",
      "epoch : 568 iter : 56 (20m 49s) 0.0012  / 23 ✓\n",
      "epoch : 577 iter : 34 (21m 7s) 0.0011  / 8 ✓\n",
      "epoch : 586 iter : 12 (21m 26s) 0.0831  / 2 ✓\n",
      "epoch : 594 iter : 48 (21m 44s) 0.0037  / 10 ✓\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.0007\n",
    "optimizer = optim.SGD(rnn.parameters(),lr=learning_rate,momentum=0.9)\n",
    "\n",
    "all_losses = []\n",
    "start = time.time()\n",
    "counter = 0\n",
    "for epoch in range(600):  \n",
    "    current_loss = 0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        output = rnn(inputs.float())\n",
    "        labels = labels.type(torch.LongTensor).to(device)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        category = LABELS[int(labels[0])]\n",
    "\n",
    "        if counter % 500 == 0:\n",
    "            guess, guess_i = categoryFromOutput(output)\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "            print('epoch : %d iter : %d (%s) %.4f  / %s %s' % (epoch, i, timeSince(start), loss, guess, correct))\n",
    "\n",
    "        \n",
    "        counter = counter + 1\n",
    "    if counter % 100 == 0:\n",
    "        all_losses.append(current_loss / 25)\n",
    "        current_loss = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network(Модель задания \"с\"):   71.5210355987055\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "right = 0\n",
    "counter = 0\n",
    "\n",
    "rnn.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        counter = counter + 1\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)  \n",
    "        output = rnn(inputs.float())\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        category = LABELS[int(labels[0])]\n",
    "        \n",
    "        if guess == category:\n",
    "            right = right + 1\n",
    "\n",
    "\n",
    "print('Accuracy of the network(Модель задания \"с\"):  ',  (100 * right / counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличение скрытых слоев нейросети в 3 раза дает улучшенный резульат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Сгенерировать другой датасет с меньшим количеством “кадров” в серии и сравнить улучшилось или ухудшилось качество предсказания. Провести несколько таких итераций, дать свою оценку уменьшению и увеличению кадров, назвать оптимальное, на ваш взгляд, их количество. Желательно сделать так, чтобы длина последовательности передавалась как атрибут класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_subjects = list(range(0, 28)) #количество людей выполняющих действия\n",
    "training_classes = sorted([8, 10, 22, 23, 27, 21, 55, 2, 7]) #классы которые будем использовать для обучения, полный список прдставлен тут https://github.com/shahroudy/NTURGB-D\n",
    "LABELS = {x: training_classes[x] for x in range(len(training_classes))}\n",
    "training_cameras = [1, 2, 3] \n",
    "\n",
    "# max_body_true = 1\n",
    "# max_body_kinect = 1\n",
    "\n",
    "num_joint = 25\n",
    "max_frame = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action classes:  {2: 0, 7: 1, 8: 2, 10: 3, 21: 4, 22: 5, 23: 6, 27: 7, 55: 8}\n",
      "action files:  {2: 120, 7: 120, 8: 120, 10: 120, 21: 120, 22: 120, 23: 120, 27: 120, 55: 120}\n"
     ]
    }
   ],
   "source": [
    "dataset = Skeleton_Dataset(data_path=data_path, broken_files_path=broken_files_path, \n",
    "                           training_classes=training_classes,num_joint = 25, \n",
    "                           max_frame = 300, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_net(\n",
       "  (lstm): LSTM(75, 384, num_layers=2, batch_first=True)\n",
       "  (dr): Dropout2d(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=384, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hidden = 128*3\n",
    "n_joints = 25*3\n",
    "n_categories = len(LABELS)\n",
    "n_layer = 2\n",
    "rnn = LSTM_net(n_joints,n_hidden,n_categories,n_layer)\n",
    "rnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 iter : 0 (0m 0s) 2.1947  / 7 ✗ (10)\n",
      "epoch : 8 iter : 36 (0m 18s) 2.1742  / 2 ✗ (21)\n",
      "epoch : 17 iter : 14 (0m 36s) 2.1263  / 2 ✗ (7)\n",
      "epoch : 25 iter : 50 (0m 54s) 2.0687  / 22 ✗ (27)\n",
      "epoch : 34 iter : 28 (1m 12s) 2.1290  / 7 ✓\n",
      "epoch : 43 iter : 6 (1m 31s) 1.7686  / 8 ✗ (27)\n",
      "epoch : 51 iter : 42 (1m 49s) 1.7903  / 2 ✓\n",
      "epoch : 60 iter : 20 (2m 8s) 1.6968  / 7 ✗ (55)\n",
      "epoch : 68 iter : 56 (2m 27s) 1.3754  / 22 ✓\n",
      "epoch : 77 iter : 34 (2m 46s) 1.4579  / 21 ✗ (23)\n",
      "epoch : 86 iter : 12 (3m 4s) 1.2081  / 21 ✗ (22)\n",
      "epoch : 94 iter : 48 (3m 23s) 1.3660  / 7 ✓\n",
      "epoch : 103 iter : 26 (3m 42s) 1.1436  / 23 ✓\n",
      "epoch : 112 iter : 4 (4m 0s) 1.2024  / 21 ✗ (2)\n",
      "epoch : 120 iter : 40 (4m 18s) 0.9893  / 7 ✓\n",
      "epoch : 129 iter : 18 (4m 38s) 0.5203  / 55 ✓\n",
      "epoch : 137 iter : 54 (4m 56s) 1.4505  / 27 ✓\n",
      "epoch : 146 iter : 32 (5m 15s) 1.0844  / 21 ✗ (10)\n",
      "epoch : 155 iter : 10 (5m 34s) 0.7903  / 55 ✓\n",
      "epoch : 163 iter : 46 (5m 53s) 0.5272  / 21 ✓\n",
      "epoch : 172 iter : 24 (6m 13s) 0.5139  / 21 ✗ (55)\n",
      "epoch : 181 iter : 2 (6m 31s) 0.7496  / 55 ✓\n",
      "epoch : 189 iter : 38 (6m 50s) 0.8355  / 8 ✗ (23)\n",
      "epoch : 198 iter : 16 (7m 9s) 1.3142  / 10 ✗ (23)\n",
      "epoch : 206 iter : 52 (7m 28s) 0.5487  / 2 ✓\n",
      "epoch : 215 iter : 30 (7m 46s) 0.5936  / 22 ✗ (7)\n",
      "epoch : 224 iter : 8 (8m 6s) 0.5694  / 2 ✓\n",
      "epoch : 232 iter : 44 (8m 25s) 0.3997  / 10 ✓\n",
      "epoch : 241 iter : 22 (8m 44s) 0.5029  / 21 ✗ (10)\n",
      "epoch : 250 iter : 0 (9m 2s) 0.6676  / 21 ✓\n",
      "epoch : 258 iter : 36 (9m 22s) 0.6496  / 23 ✗ (7)\n",
      "epoch : 267 iter : 14 (9m 41s) 0.5473  / 55 ✓\n",
      "epoch : 275 iter : 50 (9m 59s) 0.4591  / 21 ✓\n",
      "epoch : 284 iter : 28 (10m 18s) 0.1206  / 10 ✓\n",
      "epoch : 293 iter : 6 (10m 37s) 0.1452  / 7 ✓\n",
      "epoch : 301 iter : 42 (10m 56s) 0.5082  / 8 ✗ (2)\n",
      "epoch : 310 iter : 20 (11m 14s) 0.3588  / 2 ✓\n",
      "epoch : 318 iter : 56 (11m 33s) 0.0393  / 23 ✓\n",
      "epoch : 327 iter : 34 (11m 52s) 0.1119  / 23 ✓\n",
      "epoch : 336 iter : 12 (12m 11s) 0.1693  / 2 ✓\n",
      "epoch : 344 iter : 48 (12m 30s) 0.1090  / 21 ✓\n",
      "epoch : 353 iter : 26 (12m 49s) 0.0168  / 21 ✓\n",
      "epoch : 362 iter : 4 (13m 8s) 0.0089  / 27 ✓\n",
      "epoch : 370 iter : 40 (13m 27s) 0.1446  / 10 ✓\n",
      "epoch : 379 iter : 18 (13m 45s) 0.0167  / 21 ✓\n",
      "epoch : 387 iter : 54 (14m 4s) 0.0611  / 22 ✓\n",
      "epoch : 396 iter : 32 (14m 22s) 0.2210  / 10 ✓\n",
      "epoch : 405 iter : 10 (14m 41s) 0.0172  / 7 ✓\n",
      "epoch : 413 iter : 46 (15m 0s) 0.0122  / 7 ✓\n",
      "epoch : 422 iter : 24 (15m 18s) 0.0081  / 27 ✓\n",
      "epoch : 431 iter : 2 (15m 38s) 0.0086  / 21 ✓\n",
      "epoch : 439 iter : 38 (15m 57s) 0.0022  / 55 ✓\n",
      "epoch : 448 iter : 16 (16m 15s) 0.0039  / 55 ✓\n",
      "epoch : 456 iter : 52 (16m 34s) 0.0103  / 22 ✓\n",
      "epoch : 465 iter : 30 (16m 52s) 0.5879  / 10 ✗ (21)\n",
      "epoch : 474 iter : 8 (17m 11s) 0.5349  / 22 ✓\n",
      "epoch : 482 iter : 44 (17m 30s) 0.0293  / 27 ✓\n",
      "epoch : 491 iter : 22 (17m 48s) 0.0084  / 2 ✓\n",
      "epoch : 500 iter : 0 (18m 6s) 0.0062  / 23 ✓\n",
      "epoch : 508 iter : 36 (18m 25s) 0.1175  / 10 ✓\n",
      "epoch : 517 iter : 14 (18m 43s) 0.0053  / 10 ✓\n",
      "epoch : 525 iter : 50 (19m 2s) 0.0039  / 23 ✓\n",
      "epoch : 534 iter : 28 (19m 20s) 0.2704  / 22 ✓\n",
      "epoch : 543 iter : 6 (19m 38s) 0.1909  / 8 ✓\n",
      "epoch : 551 iter : 42 (19m 57s) 0.0074  / 7 ✓\n",
      "epoch : 560 iter : 20 (20m 15s) 0.0105  / 8 ✓\n",
      "epoch : 568 iter : 56 (20m 33s) 0.0037  / 2 ✓\n",
      "epoch : 577 iter : 34 (20m 52s) 0.0025  / 8 ✓\n",
      "epoch : 586 iter : 12 (21m 10s) 0.0042  / 23 ✓\n",
      "epoch : 594 iter : 48 (21m 28s) 0.0016  / 8 ✓\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.0007\n",
    "optimizer = optim.SGD(rnn.parameters(),lr=learning_rate,momentum=0.9)\n",
    "\n",
    "all_losses = []\n",
    "start = time.time()\n",
    "counter = 0\n",
    "for epoch in range(600):  \n",
    "    current_loss = 0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        output = rnn(inputs.float())\n",
    "        labels = labels.type(torch.LongTensor).to(device)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        category = LABELS[int(labels[0])]\n",
    "\n",
    "        if counter % 500 == 0:\n",
    "            guess, guess_i = categoryFromOutput(output)\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "            print('epoch : %d iter : %d (%s) %.4f  / %s %s' % (epoch, i, timeSince(start), loss, guess, correct))\n",
    "\n",
    "        \n",
    "        counter = counter + 1\n",
    "    if counter % 100 == 0:\n",
    "        all_losses.append(current_loss / 25)\n",
    "        current_loss = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network(Модель задания \"с\"):   75.40453074433657\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "right = 0\n",
    "counter = 0\n",
    "\n",
    "rnn.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        counter = counter + 1\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)  \n",
    "        output = rnn(inputs.float())\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        category = LABELS[int(labels[0])]\n",
    "        \n",
    "        if guess == category:\n",
    "            right = right + 1\n",
    "\n",
    "\n",
    "print('Accuracy of the network(Модель задания \"с\"):  ',  (100 * right / counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уменьшение клипа до 200 кадров немного улучшела модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
